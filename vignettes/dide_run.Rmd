---
title: "Running Fits on DIDE Cluster"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Running Fits on DIDE Cluster}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)
knitr::opts_knit$set(root.dir = here::here())
```

# Overview

This Rmd document gives an overview of the steps to be taken to run model fits
on the DIDE HPC cluster, check results before collating and pushing to github. 

As a background, the fits used to be run through a series of tasks in specific
docker containers. At writing, docker can't be run locally due to virtualisation
not being enabled on this windows machine, but this will be explored again 
once/if this is enabled. 

All code chunks are set to not eval so the knitted html is just for display purposes. 
I would recommend stepping through this Rmd each time to rerun the fits, updating
parameters etc as and where needed. Sections with bash code are commented out and can
be run from within the Rmd script if bash is correctly set up, and if not then can be
run from the RStudio Terminal if set up to use Git Bash as the default for the
terminal (see Tools > Global Options > Terminal > New Terminals Open With)  

\

# Set up

All tasks for running the model fits are conducted using orderly. This includes:

1. *ecdc*

This task grabs input data from various online databases, that are then fed into
downstream processes.

2. *oxford_grt*

This task is largely redundant and is a hangover from when the oxford GRT database
was used as a model input, but this has been replaced by Google mobility data. 

3. *google_brt*

Grabs and formats Google mobility data. For countries without mobility data, we
use a Boosted regression tree model to infer mobility based on timing of govt. 
policies. 

4. *covariants*

Gets/calculates the information of when the delta variant becomes dominant, as
well as any other parameters to do with this change. This must be run even if
delta adjustments are not made as it is an input to the other tasks.

5. *vaccine_inputs*

Calculates the vaccine efficacies over time, can include delta adjustments and 
waning immunity.

6. *economist*

Gets the excess mortality data if we are doing excess death fitting.


These 6 tasks form the initial tasks that ensure all the correct data is ready
before running the actual model fits. To run these tasks, we simply need to provide
what the `date` is that we are conducting model fits for, i.e. up to what date are
we running, and also whether we are doing a `short_run`. The `short_run` argument
dictates whether a small number of trees are used in the BRT.

```{r, eval = FALSE}
date <- "2022-01-20"# "2021-12-08"

# are we doing just a short run. If so only 20 mcmc iterations are used
# and only 2 draws from the posterior. Set this to TRUE to do a quick test 
# run that everything is working basically.
short_run <- FALSE

# Are we fitting to excess mortality data?
# if yes we call the economist models from github and use the excess_mortality task
# to fit to the data
excess_mortality <- FALSE

```

These arguments are then passed to our 4/5 tasks:

```{r eval=FALSE, message=FALSE, warning=FALSE}
message("*** ECDC data")
ecdc_id <- orderly::orderly_run("ecdc", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(ecdc_id)

message("*** Covid Variants data")
variant_adjustments_id <- orderly::orderly_run("variant_adjustments", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(variant_adjustments_id)
#ideally check the calibration pdf

message("*** Calculate Vaccine Inputs")
vaccine_inputs_id <- orderly::orderly_run("vaccine_inputs_2", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(vaccine_inputs_id)
#ideally check the calibration pdf

if(excess_mortality){
  message("*** Economist Model data")
  econ_id <- orderly::orderly_run("economist", parameters = list(date=date), echo = FALSE)
  orderly::orderly_commit(econ_id)
  #ideally check the calibration pdf
} else{
  #these are not used in the excess_mortality fitting
  message("*** Oxford GRT data")
  oxford_id <- orderly::orderly_run("oxford_grt", parameters = list(date=date), echo = FALSE)
  orderly::orderly_commit(oxford_id)

  message("*** Google BRT data")
  google_id <- orderly::orderly_run("brt_google_mobility", parameters = list(date=date, short_run = short_run), echo = FALSE)
  orderly::orderly_commit(google_id)
}
```

Above, we are using `orderly::orderly_run` to run our 3 tasks in our R session, 
before then committing them to the orderly archive. Alternatively, you could run
these from the terminal using:

```{bash}
# date=2021-08-16
# ./orderly run ecdc date=$DATE
```
With these tasks run, we now begin by creating our model fit tasks. Throughout
the pandemic, we have updated the model fit tasks to include new data, new models
etc. Each of the various model fit tasks starts `lmic_reports_`, but the current 
model task is `lmic_reports_vaccine`. 

We run the the model fits on the DIDE HPC cluster. To do this we first bundle the 
orderly reports to be run using `orderly::orderly_bundle_pack`. The zip files
generated from this are then copied to the `glodide` repository, which is used to 
submit these tasks. 

```{r eval=FALSE, message=FALSE, warning = FALSE}

message("*** Creating country task bundles")

# main input parameters

# how long is the mcmc for

if(!excess_mortality){
  task <- "lmic_reports_vaccine_waning"
  parameters <- list(
    date = date,
    short_run = short_run,
    n_mcmc =  20000,
    #Should we build the require documentation+data for the fit,
    #useful if just running to hone in chains (still builds fitting.pdf+grid_out.Rds)
    document = TRUE
  )
} else{
  task <- "excess_mortality_vaccine"
  parameters <- list(
    replicates = 50,
    n_mcmc = 2500,
    n_chains = 3,
    n_burnin = 2500,
    date = date
  )
}

# get the isos
iso3cs <- grep('^[A-Z]{3}\\s*', readLines(file.path(here::here(), "countries")), value = TRUE)

# make the orderly bundles to be run on the cluster
path_bundles <- file.path(
  "Q:/COVID-Fitting/glodide/analysis/data",
          paste0(
            "raw",
            ifelse(excess_mortality, "_excess", ""),
            ifelse(short_run, "_test", "")
            ),
          date
          )
dir.create(path_bundles, showWarnings = FALSE, recursive = TRUE)

# bundle these up - this will take like 10 mins to create all the zips. 
bundles <- lapply(
  iso3cs, function(x) {
    #set country in parameters
    parameters$iso3c = x
    #pack bundle
    orderly::orderly_bundle_pack(
      path = path_bundles,
      name = task,
      parameters = parameters
    )
  }
 )

# now label these with the iso3cs and save the file paths
names(bundles) <- iso3cs
saveRDS(bundles, file.path(path_bundles, "bundles.rds"))
```

This will create an orderly task for each country that is then saved in the 
location on the server where they are to be run. At this point, open the 
`glodide` repository on the server and run through the submission script to submit
these to the cluster:

```{r eval=FALSE}
system(paste0("open ","\"","Q:/COVID-Fitting/glodide/glodide.Rproj","\""))
```

----

After the tasks have all finished, we can check to see the fits to see if they are good:  

\

# Check the fits

To do this we take the path of the finished tasks from our bundles

```{r eval=FALSE, message=FALSE, warning = FALSE}

# use the bundles paths to work out the path to the runs in derived
paths <- gsub("raw", "derived", vapply(bundles, FUN = "[[", FUN.VALUE = character(1), "path"))

# now extract the fitting.pdf files
td <- tempdir()
fits <- lapply(paths, function(x) {
  if(file.exists(x)){
    zip::unzip(
      zipfile = x, 
      files = file.path(gsub("\\.zip", "", basename(x)), "pack/fitting.pdf"), 
      exdir = td
    )
  }
})

# get the filepaths for these 
pdfs <- grep("fitting", list.files(td, full.names = TRUE, recursive = TRUE), value = TRUE)

# combine the files that are larger than 0b. Ob files are for countries that have
# no COVID-19 deaths to date and as such don't have a fitting.pdf but this file is
# created because it needs to be for orderly to finish the task
qpdf::pdf_combine(
  input = pdfs[file.size(pdfs) > 0], 
  output = file.path("fits", paste0(ifelse(excess_mortality, "excess_mortality_", "lmic_reports_vaccine_rerun_"), date, ".pdf"))
)
```

Now we can view these to work out if they look good. See the troubleshoot for more
info on what to look out for etc. If some countries need to be rerun, then work out
which countries require rerunning and rebundle those to be run again:

```{r eval=FALSE, message=FALSE, warning = FALSE}
# bundle the countries we need to rerun
iso3cs_to_rerun <- 
  c("AFG", "AZE", "BFA", "BGD", "BIH", "BLR", "BOL", "BRA", "BWA", 
"CIV", "CMR", "CRI", "CUB", "DOM", "ECU", "EGY", "ETH", "GHA", 
"GTM", "HND", "IDN", "IND", "IRQ", "JOR", "KAZ", "KEN", "KGZ", 
"LBN", "LBY", "MDA", "MEX", "MKD", "MLI", "MNE", "MRT", "MWI", 
"MYS", "NER", "NGA", "NIC", "PAK", "PER", "PHL", "PRY", "PSE", 
"ROU", "RUS", "SEN", "SLV", "SOM", "SRB", "SUR", "THA", "TUR", 
"UKR", "VEN", "ZMB", "ZWE", "CAF", "FSM", "GIN", "KIR", "MDG", 
"MDV", "MNG", "MOZ", "NPL", "PRK", "SLB", "STP", "SWZ", "TCD", 
"TKM", "TON", "WSM", "YEM", "CHL", "PAN", "URY", "ARE", "AUS", 
"AUT", "BEL", "BHR", "CAN", "CHE", "CYP", "CZE", "DEU", "DNK", 
"ESP", "EST", "FIN", "FRA", "GBR", "GRC", "HRV", "HUN", "IRL", 
"ISR", "ITA", "JPN", "KWT", "LTU", "LUX", "LVA", "NCL", "NLD", 
"NOR", "NZL", "OMN", "POL", "PRT", "PYF", "QAT", "SGP", "SVK", 
"SVN", "SWE", "USA", "ABW", "CUW", "GUM", "MAC")
#change start date
bundles_to_rerun <- lapply(
  iso3cs_to_rerun, 
  function(x) {
    #set country in parameters
    parameters$iso3c = x
    orderly::orderly_bundle_pack(
      path = path_bundles,
      name = task,
      parameters = parameters
    )
  }
)

names(bundles_to_rerun) <- iso3cs_to_rerun
saveRDS(bundles_to_rerun, file.path(path_bundles, "bundles_to_rerun.rds"))
```

Then go to `glodide` and resubmit these and repeat above to see the fits. If they
are better then replace the bundle paths in `bundles` object so we know which 
are the correct tasks to then pull back down.  

\

# Pull fits back into orderly

Once we are happy with a set of fits for each country, we need to pull them back
down off the server to local.

First read in the paths of the original bundles

```{r eval=FALSE, message=FALSE, warning = FALSE}
# get the original bundles
bundles <- readRDS(file.path(path_bundles, "bundles.rds"))
```

Depending on whether any needed to be rerun, the below will be needed to 
replace paths in bundles, with the file paths of any new runs. For example, 
below I did two reruns ('bundles_to_rerun.rds' and 'BGD_to_rerun.rds'), which
I will use to replace their corresponding countries with in `bundles`:

```{r eval=FALSE, message=FALSE, warning = FALSE}

# any that we had to rerun
rerun <-  readRDS(file.path(path_bundles, "bundles_to_rerun.rds"))

# replace the ones we rerun within bundles
nms_ch <- c(names(rerun))
bundles[nms_ch] <- c(rerun)

keep <- c("AGO", "ALB", "ARG", "ARM", "BDI", "BEN", "BGR", "BLZ",
          "COD", "COG", "COL", "CPV", "DJI", "DZA", "GAB", "GEO",
          "GMB", "GUY", "HTI", "JAM", "KOR", "LBR", "LKA",
          "MAR", "MMR", "MUS", "SDN", "SYR", "TGO", "TUN", "TZA",
          "UZB", "ZAF", "BTN", "COM", "ERI", "FJI", "GNB", "GNQ",
          "GRD", "KHM", "LAO", "LCA", "LSO", "NAM", "PNG", "RWA",
          "SLE", "SSD", "TJK", "TLS", "UGA", "VCT", "VNM", "VUT",
          "CHN", "GUF", "ATG", "BHS", "BRB", "BRN", "IRN", "ISL",
          "MLT", "SAU", "SYC", "TTO", "HKG", "TWN")
library(purrr)
bundles <- map(keep, ~bundles[[.x]])

# now get the filepaths of the tasks to import back
tasks <- gsub("raw", "derived", as.character(vapply(bundles, "[[", character(1), "path")))
```

Now that we have updated with the rerun bundles, we know import these back into 
our local `orderly` archive:

```{r eval=FALSE, message=FALSE, warning = FALSE}
import <- lapply(tasks, orderly::orderly_bundle_import)

# just double check they all imported
all(unlist(import))
```

# Save on One-Drive

```{r eval=FALSE, message=FALSE, warning = FALSE}
save_one_drive <- function(){
  #this repo
  repo <- "C:\\Users\\gbarnsle\\Documents\\Covid Vaccine Impact\\global-lmic-reports-orderly"
  #my one drive folder
  destination <- file.path(
    "C:\\Users\\gbarnsle\\OneDrive - Imperial College London\\nimue_model_fits",
    ifelse(excess_mortality, "excess", "standard")
  )
  #get the fits
  fits <- squire.page::get_fits(repo = repo, date = date, iso3cs = NULL, excess = excess_mortality)
  #upload to folder replacing existing files
  dir.create(destination, recursive = TRUE, showWarnings = FALSE)
  for (iso in names(fits)) {
    saveRDS(fits[[iso]],
            file.path(
              destination,
              paste0(iso, ".Rds")
            ))
  }
  #create zip file of these fits
  zip::zip(
    root = destination,
    zipfile = paste0(date, ".zip"),
    files = paste0(names(fits), ".Rds")
  )
}
save_one_drive()
```

Actual model fits get saved to a one-drive folder for easy sharing, not likely to useful to anybody outside Imperial.

# For Excess fits and LMIC with no documentation

Update par_inits.rds and do not push to github.

```{r eval=FALSE, message=FALSE, warning = FALSE}
if(
  ifelse(
    excess_mortality,
    TRUE,
    !parameters$document
  )
){
  #set up task dependant variables
  object_name <- ifelse(excess_mortality, "res.rds", "grid_out.rds")
  date_name <- ifelse(excess_mortality, "week_start", "date")
  #get ids
  ids <- rep(NA, length(tasks))
  for(i in seq_along(tasks)){
    ids[i] <- stringr::str_remove(
      tail(
        stringr::str_split(tasks[i], "\\\\")[[1]],
        1
        ),
      ".zip"
      )
  }
  
  # get old conditions
  pars_init <- readRDS(file.path("src", task, "pars_init.rds"))
  for(x in seq_along(ids)) {
    out <- readRDS(file.path("archive", task, ids[x], object_name)
    )
    if(!is.null(out)){
      if("pmcmc_results" %in% names(out)) {
        if("chains" %in% names(out$pmcmc_results)) {
          mc <- do.call(rbind, lapply(out$pmcmc_results$chains, "[[", "results"))
        } else {
          mc <- out$pmcmc_results$results
        }
        best <- mc[which.max(mc$log_posterior),]
        best <- best[,seq_len(ncol(best)-3)]
        rownames(best) <- NULL
        best$start_date <- as.character(
          squire:::offset_to_start_date(
            out$pmcmc_results$inputs$data[[date_name]][1],
            round(best$start_date)
            )
          )
        best$iso3c <- countrycode::countrycode(
          out$parameters$country, origin = "country.name", destination = "iso3c"
          )
        best$Rt_rw_duration <- out$pmcmc_results$inputs$Rt_args$Rt_rw_duration
  
        if("chains" %in% names(out$pmcmc_results)) {
          best$covariance_matrix <- out$pmcmc_results$chains$chain1$covariance_matrix[1]
          best$scaling_factor <- mean(
            c(tail(na.omit(out$pmcmc_results$chains$chain1$scaling_factor),1),
              tail(na.omit(out$pmcmc_results$chains$chain2$scaling_factor),1),
              tail(na.omit(out$pmcmc_results$chains$chain3$scaling_factor),1))
              )
        } else {
          best$covariance_matrix <- out$pmcmc_results$covariance_matrix[1]
          best$scaling_factor <- tail(na.omit(out$pmcmc_results$scaling_factor),1)
        }
  
        # now replace par inits for the relevant country
        pars_init[[best$iso3c]] <- best
      }
    } 
  }
  saveRDS(pars_init, file.path("src", task, "pars_init.rds"))
}
```

# Create the changes to the github pages

## 1. Get gh-pages

The Github pages repository is at [www.github.com:mrc-ide/global-lmic-reports](www.github.com:mrc-ide/global-lmic-reports) and
contains all the outputs of model fits conducted to date and the web pages to describe the model fits for each country. It is
cloned into `gh-pages`, which is where we collate the outputs of the model fits into. 

If gh-pages is not in the repo, i.e. you have just clones `global-lmic-reports-orderly` down for example, then you need to run following in terminal (or from within this repo if the bash chunk tag is working correctly).

```{bash}
# mkdir gh-pages
# git clone git@github.com:mrc-ide/global-lmic-reports.git gh-pages
```

## 2. The collation step for run_collate_vaccine

Next we run the collation step, where the necessary outputs from the model fits are copied to gh-pages. It conducts the following steps:

1. Fetches the latest run model fit for each country for the date argument provided. 
2. Updates `pars_init.rds` in `lmic_reports_vaccine_waning` with parameters from best model fit for each country.
3. Copies the pdfs/html files over to `gh-pages`
4. Copies the projections from each model fit over and combines these into a combined data zip
5. Copies model projections to the `index_page` and `regional_page` tasks
6. Runs the remaining tasks needed to create the website (index, FAQ, regional pages)
7. Copies the output from these remaining tasks to gh-pages. 

Things to note before running. In step 2, we overwrite the `pars_init.rds` object in `lmic_reports_vaccine_waning`. This is so that next time this task is run, we are using the best initial conditions for the next time you run `lmic_reports_vaccine_waning`. As a result of this overwrite, be careful to make sure you are only calling this collation once you are happy you have the right model fits (and if not make sure `pars_init.rds` is backed up/committed).

Either execute this from the terminal or you can run it using the bash code chunk below (though it doesn't produce output until after it has finished at the moment, which may be annoying so you can see the output). 
```{bash}
 ./run/run_collate_vaccine_waning.sh 2022-01-02
```

## 3. Push the results to the repository 

Once we are happy that the collation has worked we need to push this to Github. We
do this in two steps as the git pack size if all done at once is too big. This is all in 
the following script:

```{bash}
# ./scripts/publish_website production
```

