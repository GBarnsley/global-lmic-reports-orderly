---
title: "Running Fits on DIDE Cluster"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Running Fits on DIDE Cluster}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)
knitr::opts_knit$set(root.dir = here::here())
```

# Overview

This Rmd document gives an overview of the steps to be taken to run model fits
on the DIDE HPC cluster, check results before collating and pushing to github. 

As a background, the fits used to be run through a series of tasks in specific
docker containers. At writing, docker can't be run locally due to virtualisation
not being enabled on this windows machine, but this will be explored again 
once/if this is enabled. 

All code chunks are set to not eval so the knitted html is just for display purposes. 
I would recommend stepping through this Rmd each time to rerun the fits, updating
parameters etc as and where needed. Sections with bash code are commented out and can
be run from within the Rmd script if bash is correctly set up, and if not then can be
run from the RStudio Terminal if set up to use Git Bash as the default for the
terminal (see Tools > Global Options > Terminal > New Terminals Open With)  

\

# Set up

All tasks for running the model fits are conducted using orderly. This includes:

1. *ecdc*

This task grabs input data from various online databases, that are then fed into
downstream processes. 

2. *oxford_grt*

This task is largely redundant and is a hangover from when the oxford GRT database
was used as a model input, but this has been replaced by Google mobility data. 

3. *google_brt*

Grabs and formats Google mobility data. For countries without mobility data, we
use a Boosted regression tree model to infer mobility based on timing of govt. 
policies. 

These 3 tasks form the initial tasks that ensure all the correct data is ready
before running the actual model fits. To run these tasks, we simply need to provide
what the `date` is that we are conducting model fits for, i.e. up to what date are
we running, and also whether we are doing a `short_run`. The `short_run` argument
dictates whether a small number of trees are used in the BRT.

```{r, eval = FALSE}
# date <- "2021-08-20"
```

These arguments are then passed to our 3 tasks:

```{r eval=FALSE, message=FALSE, warning=FALSE}
message("*** ECDC data")
ecdc_id <- orderly::orderly_run("ecdc", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(ecdc_id)

message("*** Oxford GRT data")
oxford_id <- orderly::orderly_run("oxford_grt", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(oxford_id)

message("*** Google BRT data")
google_id <- orderly::orderly_run("brt_google_mobility", parameters = list(date=date, short_run = short_run), echo = FALSE)
orderly::orderly_commit(google_id)
```

Above, we are using `orderly::orderly_run` to run our 3 tasks in our R session, 
before then committing them to the orderly archive. Alternatively, you could run
these from the terminal using:

```{bash}
# date=2021-08-16
# ./orderly run ecdc date=$DATE
```
With these tasks run, we now begin by creating our model fit tasks. Throughout
the pandemic, we have updated the model fit tasks to include new data, new models
etc. Each of the various model fit tasks starts `lmic_reports_`, but the current 
model task is `lmic_reports_vaccine`. 

We run the the model fits on the DIDE HPC cluster. To do this we first bundle the 
orderly reports to be run using `orderly::orderly_bundle_pack`. The zip files
generated from this are then copied to the `glodide` repository, which is used to 
submit these tasks. 

```{r eval=FALSE, message=FALSE, warning = FALSE}

message("*** Creating country task bundles")

# main input parameters

# how long is the mcmc for
n_mcmc <- 20000

# should tasks be run in parallel and use multiple chains. 
# leave this as FALSE and see FAQs for more info on this
parallel <- FALSE

# are we doing just a short run. If so only 20 mcmc iterations are used
# and only 2 draws from the posterior. Set this to TRUE to do a quick test 
# run that everything is working basically.
short_run <- FALSE

# not needed input parameters - leave as FALSE. See FAQ for more info on these
full_scenarios <- FALSE
gibbs_sampling <- FALSE

# get the isos
iso3cs <- grep('^[A-Z]{3}\\s*', readLines(file.path(here::here(), "countries")), value = TRUE)

# make the orderly bundles to be run on the cluster
test <- short_run
if(test) {
path_bundles <- file.path("L:/OJ/glodide/analysis/data","raw_test", date)
} else {
  path_bundles <- file.path("L:/OJ/glodide/analysis/data", "raw", date)
}
dir.create(path_bundles, showWarnings = FALSE)

# bundle these up - this will take like 10 mins to create all the zips. 
bundles <- lapply(
  iso3cs, function(x) {
    orderly::orderly_bundle_pack(
      path = path_bundles,
      name = "lmic_reports_vaccine",
      parameters = list(
        iso3c = x,
        date=date,
        short_run=short_run,
        parallel=parallel,
        full_scenarios=full_scenarios,
        gibbs_sampling=gibbs_sampling,
        n_mcmc=n_mcmc
      )
    )
  }
)

# now label these with the iso3cs and save the file paths
names(bundles) <- iso3cs
saveRDS(bundles, file.path(path_bundles, "bundles.rds"))

```

This will create an orderly task for each country that is then saved in the 
location on the server where they are to be run. At this point, open the 
`glodide` repository on the server and run through the submission script to submit
these to the cluster:

```{r eval=FALSE}
system(paste0("open ","\"","L:/OJ/glodide/glodide.Rproj","\""))
```

----

After the tasks have all finished, we can check to see the fits to see if they are good:  

\

# Check the fits

To do this we take the path of the finished tasks from our bundles

```{r eval=FALSE, message=FALSE, warning = FALSE}

# use the bundles paths to work out the path to the runs in derived
paths <- gsub("raw", "derived", vapply(bundles, FUN = "[[", FUN.VALUE = character(1), "path"))

# now extract the fitting.pdf files
td <- tempdir()
fits <- lapply(paths, function(x) {
  if(file.exists(x)){
    zip::unzip(
      zipfile = x, 
      files = file.path(gsub("\\.zip", "", basename(x)), "pack/fitting.pdf"), 
      exdir = td
    )
  }
})

# get the filepaths for these 
pdfs <- grep("fitting", list.files(td, full.names = TRUE, recursive = TRUE), value = TRUE)

# combine the files that are larger than 0b. Ob files are for countries that have
# no COVID-19 deaths to date and as such don't have a fitting.pdf but this file is
# created because it needs to be for orderly to finish the task
qpdf::pdf_combine(
  input = pdfs[file.size(pdfs) > 0], 
  output = file.path("fits", paste0("lmic_reports_vaccine_", date, ".pdf"))
)
```

Now we can view these to work out if they look good. See the troubleshoot for more
info on what to look out for etc. If some countries need to be rerun, then work out
which countries require rerunning and rebundle those to be run again:

```{r eval=FALSE, message=FALSE, warning = FALSE}
# bundle the countries we need to rerun
iso3cs_to_rerun <- c("BGD") 
bundles_to_rerun <- lapply(
  iso3cs_to_rerun, 
  function(x) {
    orderly::orderly_bundle_pack(
      path = path_bundles,
      name = "lmic_reports_vaccine",
      parameters = list(
        iso3c = x,
        date=date,
        short_run=short_run,
        parallel=parallel,
        full_scenarios=full_scenarios,
        gibbs_sampling=gibbs_sampling,
        n_mcmc=n_mcmc
      )
    )
  }
)

names(bundles_to_rerun) <- iso3cs_to_rerun
saveRDS(bundles_to_rerun, file.path(path_bundles, "BGD_to_rerun.rds"))

```

Then go to `glodide` and resubmit these and repeat above to see the fits. If they
are better then replace the bundle paths in `bundles` object so we know which 
are the correct tasks to then pull back down.  

\

# Pull fits back into orderly

Once we are happy with a set of fits for each country, we need to pull them back
down off the server to local.

First read in the paths of the original bundles

```{r eval=FALSE, message=FALSE, warning = FALSE}
# get the original bundles
bundles <- readRDS(file.path(path_bundles, "bundles.rds"))
```

Depending on whether any needed to be rerun, the below will be needed to 
replace paths in bundles, with the file paths of any new runs. For example, 
below I did two reruns ('bundles_to_rerun.rds' and 'BGD_to_rerun.rds'), which
I will use to replace their corresponding countries with in `bundles`:

```{r eval=FALSE, message=FALSE, warning = FALSE}

# any that we had to rerun
rerun <- readRDS(file.path(path_bundles, "bundles_to_rerun.rds"))
bgd_rerun <- readRDS(file.path(path_bundles, "BGD_to_rerun.rds"))

# replace the ones we rerun within bundles
nms_ch <- c(names(rerun), names(bgd_rerun))
bundles[nms_ch] <- c(rerun, bgd_rerun)

# now get the filepaths of the tasks to import back
tasks <- gsub("raw", "derived", as.character(vapply(bundles, "[[", character(1), "path")))
```

Now that we have updated with the rerun bundles, we know import these back into 
our local `orderly` archive:

```{r eval=FALSE, message=FALSE, warning = FALSE}
import <- lapply(tasks, orderly::orderly_bundle_import)

# just double check they all imported
all(unlist(import))
```

\

# Create the changes to the github pages

## 1. Get gh-pages

The Github pages repository is at [www.github.com:mrc-ide/global-lmic-reports](www.github.com:mrc-ide/global-lmic-reports) and
contains all the outputs of model fits conducted to date and the web pages to describe the model fits for each country. It is
cloned into `gh-pages`, which is where we collate the outputs of the model fits into. 

If gh-pages is not in the repo, i.e. you have just clones `global-lmic-reports-orderly` down for example, then you need to run following in terminal (or from within this repo if the bash chunk tag is working correctly).

```{bash}
# mkdir gh-pages
# git clone git@github.com:mrc-ide/global-lmic-reports.git gh-pages
```

## 2. The collation step for run_collate_vaccine

Next we run the collation step, where the necessary outputs from the model fits are copied to gh-pages. It conducts the following steps:

1. Fetches the latest run model fit for each country for the date argument provided. 
2. Updates `pars_init.rds` in `lmic_reports_vaccine` with parameters from best model fit for each country.
3. Copies the pdfs/html files over to `gh-pages`
4. Copies the projections from each model fit over and combines these into a combined data zip
5. Copies model projections to the `index_page` and `regional_page` tasks
6. Runs the remaining tasks needed to create the website (index, FAQ, regional pages)
7. Copies the output from these remaining tasks to gh-pages. 

Things to note before running. In step 2, we overwrite the `pars_init.rds` object in `lmic_reports_vaccine`. This is so that next time this task is run, we are using the best initial conditions for the next time you run `lmic_reports_vaccine`. As a result of this overwrite, be careful to make sure you are only calling this collation once you are happy you have the right model fits (and if not make sure `pars_init.rds` is backed up/committed).

Either execute this from the terminal or you can run it using the bash code chunk below (though it doesn't produce output until after it has finished at the moment, which may be annoying so you can see the output). 
```{bash}
# ./run/run_collate_vaccine.sh 2021-08-19
```

## 3. Push the results to the repository 

Once we are happy that the collation has worked we need to push this to Github. We
do this in two steps as the git pack size if all done at once is too big. This is all in 
the following script:

```{bash}
# ./scripts/publish_website production
```

